# MambaInVisualRingAttention
An implementation of LWM and Mamba in ring attention to achieve near infinite context length.



This is just a test for now.
If this technique does not work to achieve infinite context length, at least we can achieve the same performance as LWM.

Another possible implementation would be using the mamba module from the following repo:
https://github.com/jzhang38/LongMamba/blob/main/experimental/mamba_module.py

I would also like to implement this into a mixture of experts model. Hopefully Google doesnt beat me to it.

## Citation


If you find this work useful in your research, please consider funding my company Moins Tech (Less Tech). You can find us at https://moins.tech/